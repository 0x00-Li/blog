# 环境准备

## 模板镜像

基础环境使用centos7

参考文献：https://www.cnblogs.com/zhaopengcheng/p/13582933.html

### 安装依赖

`yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vimnet-tools git`

### 设置防火墙为 Iptables 并设置空规则

停止并禁用防火墙

`systemctl stop firewalld && systemctl disable firewalld`

安装iptables，并设置相关基础配置

`yum -y install iptables-services && systemctl start iptables && systemctl enable iptables && iptables -F && service iptables save`



### 关闭SELINUX

修改`/etc/selinux/config`文件，将SELINUX 设置为disabled

```shell
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected. 
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted

```

### 关闭swap

`swapoff -a ; sed -i '/swap/d' /etc/fstab`

### 修改国内源

> https://www.cnblogs.com/zhaopengcheng/p/13582933.html

```shell
# 备份本地yum源
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak 

# 获取阿里yum源配置
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

# 更新catch
yum clean all # 清除系统所有的yum缓存 
yum makecache # 生成yum缓存

# 查看
yum -y update 

# 调整时区为上海
timedatectl set-timezone Asia/Shanghai
```

### 关闭不需要的服务

`systemctl stop postfix && systemctl disable postfix`

### 使用本地软件包管理软件安装kubectl

```shell
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
```



### 升级内核

CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定

```shell
# 获取源
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm
 
# 安装，装完成后检查 /boot/grub2/grub.cfg中对应内核menuentry中是否包含 initrd16 配置，如果没有，再安装一次！
yum --enablerepo=elrepo-kernel install -y kernel-lt 

# 查看系统全部内核
sudo awk -F\' '$1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg
# 控制台输出
# 0 : CentOS Linux (5.4.144-1.el7.elrepo.x86_64) 7 (Core)
# 1 : CentOS Linux (3.10.0-1160.41.1.el7.x86_64) 7 (Core)
# 2 : CentOS Linux (3.10.0-1160.el7.x86_64) 7 (Core)
# 3 : CentOS Linux (0-rescue-d42bcfdb75c63e4ab3c18ed2ec588f9c) 7 (Core)

# 设置开机从新内核启动
grub2-set-default 0
# 生成grub配置文件
grub2-mkconfig -o /boot/grub2/grub.cfg

# 重启使配置有效
reboot
```



## Docker 安装

### 安装所需的软件包

yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2。

```shell
sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
```

### 增加docker国内源

```shell
yum-config-manager \
    --add-repo \
    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

### 安装最新版本的 Docker Engine-Community 和 containerd

`sudo yum install docker-ce docker-ce-cli containerd.io`

执行后提示：

```shell
没有可用软件包 docker-ce。
没有可用软件包 docker-ce-cli。
没有可用软件包 containerd.io。
错误：无须任何处理
```





> 网友提供的解决方案：https://blog.csdn.net/ctm526926722/article/details/115037474
>
> `yum-config-manager --add-repo   http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo`
>
> 然后重新执行安装命令

### 配置daemon

创建docker目录

`mkdir /etc/docker`

配置daemon文件

```json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
```

`mkdir -p /etc/systemd/system/docker.service.d`

```shell
# Restart Docker
systemctl daemon-reload
systemctl restart docker

# 开机启动docker
systemctl enable docker
# 查看docker版本
docker --version
# Docker version 20.10.8, build 3967b7d
```

## 安装kube相关工具

根据安装的版本查询后，支持的 k8s 版本为 `1.21.4` [查询方法](https://www.cnblogs.com/sylvia-liu/p/14884920.html)

`yum install -y kubelet-1.21.4 kubeadm-1.21.4 kubectl-1.21.4`

设置开机启动

`systemctl enable kubelet && systemctl start kubelet`

查看相关版本

```
kubeadm version
kubectl version --client
kubelet --version
```



### 使用桥接流量，对iptables 可见

编辑文件`/etc/sysctl.d/k8s.conf`

```shell
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
```

验证是否生效，都返回1即正确

```shell
sysctl -n net.bridge.bridge-nf-call-iptables
sysctl -n net.bridge.bridge-nf-call-ip6tables
```

开启流量都过iptables?

`echo "1" >/proc/sys/net/ipv4/ip_forward`



## 创建集群

### 及其互联

在192.168.1.200 上执行 `hostnamectl set-hostname k8s-master`

在192.168.1.201 上执行 `hostnamectl set-hostname k8s-node1`

在192.168.1.202 上执行 `hostnamectl set-hostname k8s-node2`

并设置自身的回环hosts`127.0.0.1 k8s-master` 每个节点根据自己的hostname执行



在三个节点上设置hosts

```
192.168.1.200 k8s-master
192.168.1.201 k8s-node1
192.168.1.202 k8s-node2
```





### 初始化Master

```shell
kubeadm init --kubernetes-version=1.21.4 \
--apiserver-advertise-address=192.168.1.200 \
--ignore-preflight-errors=all \
--image-repository registry.aliyuncs.com/google_containers \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.244.0.0/16
```

POD的网段为: 10.122.0.0/16， api server地址就是master本机IP

> 这一步很关键，由于kubeadm 默认从官网k8s.grc.io下载所需镜像，国内无法访问，因此需要通过–image-repository指定阿里云镜像仓库地址

参数介绍：

```shell
–kubernetes-version: 用于指定k8s版本；
–apiserver-advertise-address：用于指定kube-apiserver监听的ip地址,就是 master本机IP地址。
–pod-network-cidr：用于指定Pod的网络范围； 10.244.0.0/16
–service-cidr：用于指定SVC的网络范围；
–image-repository: 指定阿里云镜像仓库地址
```

整个日志的输出：

```shell
[init] Using Kubernetes version: v1.21.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/coredns:v1.8.0: output: Error response from daemon: manifest for registry.aliyuncs.com/google_containers/coredns:v1.8.0 not found: manifest unknown: manifest unknown
, error: exit status 1
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.1.200]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.1.200 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.1.200 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 18.505710 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 03gb7q.owuk8b1p7hcw7cp4
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.200:6443 --token 03gb7q.owuk8b1p7hcw7cp4 \
	--discovery-token-ca-cert-hash sha256:1745065c94589126fa98dceced7bab0cb4210b319e99dfae41264922e5838b13
```

查看kubelet 运行状态，已经是running ，启动成功

```
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since 日 2021-09-05 20:29:26 CST; 3min 57s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 3471 (kubelet)
    Tasks: 13
   Memory: 51.8M
   CGroup: /system.slice/kubelet.service
           └─3471 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/v...

9月 05 20:32:58 k8s-master kubelet[3471]: E0905 20:32:58.689163    3471 kubelet.go:2211] "Container runtime network not ready" networkReady="...ialized"
9月 05 20:33:01 k8s-master kubelet[3471]: I0905 20:33:01.452959    3471 cni.go:239] "Unable to update cni config" err="no networks found in /...i/net.d"
9月 05 20:33:03 k8s-master kubelet[3471]: E0905 20:33:03.699145    3471 kubelet.go:2211] "Container runtime network not ready" networkReady="...ialized"
9月 05 20:33:06 k8s-master kubelet[3471]: I0905 20:33:06.453298    3471 cni.go:239] "Unable to update cni config" err="no networks found in /...i/net.d"
9月 05 20:33:08 k8s-master kubelet[3471]: E0905 20:33:08.717903    3471 kubelet.go:2211] "Container runtime network not ready" networkReady="...ialized"
9月 05 20:33:11 k8s-master kubelet[3471]: I0905 20:33:11.453905    3471 cni.go:239] "Unable to update cni config" err="no networks found in /...i/net.d"
9月 05 20:33:13 k8s-master kubelet[3471]: E0905 20:33:13.729625    3471 kubelet.go:2211] "Container runtime network not ready" networkReady="...ialized"
9月 05 20:33:16 k8s-master kubelet[3471]: I0905 20:33:16.458773    3471 cni.go:239] "Unable to update cni config" err="no networks found in /...i/net.d"
9月 05 20:33:18 k8s-master kubelet[3471]: E0905 20:33:18.740938    3471 kubelet.go:2211] "Container runtime network not ready" networkReady="...ialized"
9月 05 20:33:21 k8s-master kubelet[3471]: I0905 20:33:21.459225    3471 cni.go:239] "Unable to update cni config" err="no networks found in /...i/net.d"
Hint: Some lines were ellipsized, use -l to show in full.
```

查看每个组件的状态：

`kubectl get cs`

```
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"} 
```

查看节点

`kubectl get pod --all-namespaces -o wide`

```
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-59d64cd4d4-h8mbj             0/1     Pending   0          7m29s   <none>          <none>       <none>           <none>
kube-system   coredns-59d64cd4d4-hw6dd             0/1     Pending   0          7m29s   <none>          <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          7m36s   192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          7m44s   192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          7m36s   192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-proxy-xc427                     1/1     Running   0          7m29s   192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          7m36s   192.168.1.200   k8s-master   <none>           <none>

```

添加网络

```
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

或者

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

这里我们选择第二种，flannel

`kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml`

> 直接从github上下载失败
>
> 我们直接从github上下载文件，使用手动执行文件的方式`kubectl apply -f ./kube-flannel.yml`

在worker节点，执行命令，加入集群

```
kubeadm join 192.168.1.200:6443 --token 03gb7q.owuk8b1p7hcw7cp4 \
	--discovery-token-ca-cert-hash sha256:1745065c94589126fa98dceced7bab0cb4210b319e99dfae41264922e5838b13
```

在执行这个命令的时候，报错：

```
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

```

在之前已经执行过的`echo '1'> /proc/sys/net/ipv4/ip_forward` 在重启的时候，失效了，在此重新执行下，然后执行加入集群命令即可



在master上重新查看节点状态

执行`kubectl get pod --all-namespaces -o wide`

```
kube-system   coredns-59d64cd4d4-h8mbj             0/1     ImagePullBackOff   0          17m     10.244.0.3      k8s-master   <none>           <none>
kube-system   coredns-59d64cd4d4-hw6dd             0/1     ImagePullBackOff   0          17m     10.244.0.2      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running            0          17m     192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running            0          17m     192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running            0          17m     192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-4rln7                1/1     Running            0          5m23s   192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-pvs4b                1/1     Running            0          85s     192.168.1.202   k8s-node2    <none>           <none>
kube-system   kube-flannel-ds-vlcqp                1/1     Running            0          2m20s   192.168.1.201   k8s-node1    <none>           <none>
kube-system   kube-proxy-k4wgc                     1/1     Running            0          2m20s   192.168.1.201   k8s-node1    <none>           <none>
kube-system   kube-proxy-ngzkr                     1/1     Running            0          85s     192.168.1.202   k8s-node2    <none>           <none>
kube-system   kube-proxy-xc427                     1/1     Running            0          17m     192.168.1.200   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running            0          17m     192.168.1.200   k8s-master   <none>           <none>
```

从上面的状态可以看到 **coredns 处于 ImagePullBackOff状态** 根据[网友方案](https://blog.csdn.net/weifangwei100/article/details/118940876)

方案中需要手动拉取coredns镜像，先将docker镜像源配置为国内加速源[docker 镜像国内加速](https://blog.csdn.net/qq_37949192/article/details/117815701)

编辑`/etc/docker/daemon.json`增加

`"registry-mirrors": ["http://hub-mirror.c.163.com"]`





可用的加速有：

```
Docker中国区官方镜像 https://registry.docker-cn.com
网易http://hub-mirror.c.163.com
ustc https://docker.mirrors.ustc.edu.cn
中国科技大学https://docker.mirrors.ustc.edu.cn
阿里云容器  服务 https://cr.console.aliyun.com/
```



配置完成后，需要重启docker服务`systemctl restart docker.service`，但是k8s集群正在运行



至此修复完成



查看节点`kubectl get node`

```
NAME         STATUS   ROLES                  AGE   VERSION
k8s-master   Ready    control-plane,master   70m   v1.21.4
k8s-node1    Ready    <none>                 54m   v1.21.4
k8s-node2    Ready    <none>                 53m   v1.21.4
```

发现两个node节点 没有roles

首先将master节点的/etc/kubernetes/admin.conf 分别复制到两外两个几点

`scp /etc/kubernetes/admin.conf root@k8s-node1:/etc/kubernetes/admin.conf`



删除节点

`kubectl delete node k8s-node    ---k8s-node节点名称，当然不只这一种删除pod的方法，我这里不一一列出了`

在k8s-node1 执行重新加入的时候报错：

```
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```

> 加入集群时报错：/etc/kubernetes/kubelet.conf already exists
>
> 原因：上次的配置文件没有清理干净，删除即可
>
> `rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ca.crt`

>   加入集群时报错： [ERROR Port-10250]: Port 10250 is in use
>
> 原因：上次加入没有成功就关闭。重置kubeadm
>
> ```
> kubeadm reset
> ```

## 示例

### 安装dashboard

下载dashboard.yml 文件

`https://github.com/kubernetes/dashboard/blob/v2.0.0/aio/deploy/recommended.yaml`

部署

```
kubectl create -f dashboard.yaml
kubectl proxy
```

可以通过一下方式访问dashborad

`https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md`



### 设置可以从外部直接访问

设置可以在外部访问dashboard，修改 dashboard以 nodePort 访问，编辑配置文件

`kubectl -n kubernetes-dashboard edit service kubernetes-dashboard`

```
修改类型
type: ClusterIP 
改为
type: NodePort
```



查看暴露端口

```
kubectl -n kubernetes-dashboard get service kubernetes-dashboard
```

```
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.1.30.215   <none>        443:30535/TCP   4m32s

```

浏览器打开，提示

![](E:\ldm-git\blog\tech\kubernetes\img\dashboard_login.png)

查看dashboard.yaml

![](E:\ldm-git\blog\tech\kubernetes\img\dashboard-account.png)

表示创建了kubernetes-dashboard账户



为账户创建token

`kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep kubernetes-dashboard | awk '{print $1}')`

输出

```
Name:         kubernetes-dashboard-certs
Namespace:    kubernetes-dashboard
Labels:       k8s-app=kubernetes-dashboard
Annotations:  <none>

Type:  Opaque

Data
====


Name:         kubernetes-dashboard-csrf
Namespace:    kubernetes-dashboard
Labels:       k8s-app=kubernetes-dashboard
Annotations:  <none>

Type:  Opaque

Data
====
csrf:  256 bytes


Name:         kubernetes-dashboard-key-holder
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
priv:  1675 bytes
pub:   459 bytes


Name:         kubernetes-dashboard-token-kl8wm
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: kubernetes-dashboard
              kubernetes.io/service-account.uid: 4e7a8a13-655c-4c1c-bf4c-6558c22c8d9f

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6Ikc1VXJidUFmTEYzUUFUcmg4MVZ3a1hKMGFvQXhvM0ExTU45SFFVQ0Z5WVkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1rbDh3bSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjRlN2E4YTEzLTY1NWMtNGMxYy1iZjRjLTY1NThjMjJjOGQ5ZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.WGSxHvffK9vOxkxPsVjgNHiNUklgskZAhu0eLOYZFcI_PVlAfznUtfOoFibq3s4hGvoAhcupGlLQX6j3q3cmtmcR959Ag-VIQsg60Hbdw9YfE5f5I7k9X7rzj9F97t2H_pvJzkf2ABsDeHnsR5JRAPPUeGxTk3d3yfsTidfYbBc_sWOO1tkzt-KyigoFgmXoTiuRHWdQBJD9Hyz1FddpAZpF8P8y6S6Q2HJv5ih-8yohBso-PpdpF8x1hBDvoDqWr_yjhFGmw0LRrarcAkuLj6LRCKTL26-NTl_TZQQC1bnGNRWD2E6AcyvPHauNcE2NLeD-1CjHse11uWmR3K1Cmg

```

使用token登录查看，没有具体运行内容，所以什么都看不出来



## 安装一个nginx试试

> https://www.cnblogs.com/saneri/p/14463509.html

编写一个nginx-deployment.yaml，

deployment 中需要依赖具体的镜像，我们直接先拉取镜像nginx

`docker pull nginx`



内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zero-test-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: ngx-service
  labels:
    app: nginx
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 32500
```

查看pod和svc服务

```
NAME                                  READY   STATUS              RESTARTS   AGE
pod/zero-test-nginx-585449566-chb69   0/1     ContainerCreating   0          6m

NAME                  TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
service/kubernetes    ClusterIP   10.1.0.1     <none>        443/TCP        93m
service/ngx-service   NodePort    10.1.27.10   <none>        80:32500/TCP   6m

```

pod 一直处于 **ContainerCreating**状态

查看错误信息`kubectl describe pod pod/zero-test-nginx-585449566-chb69 `

报错信息：

```
  Normal   Scheduled               4m29s                   default-scheduler  Successfully assigned default/zero-test-nginx-585449566-chb69 to k8s-node1
  Warning  FailedMount             4m28s                   kubelet            MountVolume.SetUp failed for volume "kube-api-access-pw7qd" : failed to sync configmap cache: timed out waiting for the condition
  Warning  FailedCreatePodSandBox  4m27s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "1875e27def0599d75244ae5045913cd130ea92b8bea6ec9a79c63a99a6d3f3b6" network for pod "zero-test-nginx-585449566-chb69": networkPlugin cni failed to set up pod "zero-test-nginx-585449566-chb69_default" network: failed to set bridge addr: "cni0" already has an IP address different from 10.244.3.1/24

```

检查发现node1的状态是NotReady,删除重新加入一下集群

网络不通，参考网友 https://blog.csdn.net/ibless/article/details/107899009



至此启动成功



修改副本数为2

`replicas: 2`

重新应用`kubectl apply -f nginx-deployment.yml`





## 卸载k8s[未进行验证]

```
kubectl delete node --all
kubeadm reset -f
modprobe -r ipip
lsmod
rm -rf ~/.kube/
rm -rf /etc/kubernetes/
rm -rf /etc/systemd/system/kubelet.service.d
rm -rf /etc/systemd/system/kubelet.service
rm -rf /usr/bin/kube*
rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /var/lib/etcd
rm -rf /var/etcd
yum clean all
yum remove kube*
```

